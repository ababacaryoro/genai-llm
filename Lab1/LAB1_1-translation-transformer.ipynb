{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c09f061e",
   "metadata": {},
   "source": [
    "Install required libraries : \n",
    "\n",
    "\n",
    "`!pip install \"numpy<2\" --upgrade `\n",
    "\n",
    "\n",
    "`!pip install pandas torch scikit-learn streamlit`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e28fa",
   "metadata": {},
   "source": [
    "### A Translation transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77f3d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 19.0265\n",
      "Epoch 20, Loss: 0.4420\n",
      "Epoch 40, Loss: 0.0753\n",
      "Epoch 60, Loss: 0.0275\n",
      "Epoch 80, Loss: 0.0182\n",
      "Epoch 100, Loss: 0.0132\n",
      "Epoch 120, Loss: 0.0101\n",
      "Epoch 140, Loss: 0.0080\n",
      "Epoch 160, Loss: 0.0065\n",
      "Epoch 180, Loss: 0.0054\n",
      "CPU times: user 1min 33s, sys: 6.58 s, total: 1min 40s\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ---------------------------\n",
    "# Toy Dataset: English → French\n",
    "# ---------------------------\n",
    "pairs = [\n",
    "    (\"hello\", \"bonjour\"),\n",
    "    (\"world\", \"monde\"),\n",
    "    (\"je\", \"I\"),\n",
    "    (\"ingenieur\", \"engineer\"),\n",
    "    (\"sac\", \"bag\"),\n",
    "    (\"cup\", \"tasse\"),\n",
    "]\n",
    "\n",
    "# Build vocab\n",
    "src_vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2} # Initialize Source vocabulary with special tokens\n",
    "tgt_vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2} # Initialize Target vocabulary with special tokens\n",
    "\n",
    "# Create & Populate vocabularies with characters from dataset (both source and target) => char-level tokenization & reference for embedding vectors\n",
    "for src, tgt in pairs:\n",
    "    for ch in src:\n",
    "        if ch not in src_vocab: src_vocab[ch] = len(src_vocab)\n",
    "    for ch in tgt:\n",
    "        if ch not in tgt_vocab: tgt_vocab[ch] = len(tgt_vocab)\n",
    "\n",
    "# Inverse vocabularies for decoding dictionaries (id → char)\n",
    "inv_src_vocab = {v:k for k,v in src_vocab.items()}\n",
    "inv_tgt_vocab = {v:k for k,v in tgt_vocab.items()}\n",
    "\n",
    "# Encoding and Padding functions\n",
    "def encode(text, vocab):\n",
    "    return [vocab[\"<sos>\"]] + [vocab[ch] for ch in text] + [vocab[\"<eos>\"]]\n",
    "\n",
    "def pad(seq, max_len):\n",
    "    return seq + [0]*(max_len-len(seq))\n",
    "\n",
    "# Parameter: Maximum sequence lengths to control the input & output sizes (adding 2 for <sos> and <eos>)\n",
    "src_max_len = max(len(s) for s,_ in pairs)+2 # Control the maximum length of the source sequences\n",
    "tgt_max_len = max(len(t) for _,t in pairs)+2 # Control the maximum length of the target sequences\n",
    "\n",
    "# Prepare dataset: Encode and pad all sequences\n",
    "data = []\n",
    "for src, tgt in pairs:\n",
    "    src_ids = pad(encode(src, src_vocab), src_max_len)\n",
    "    tgt_ids = pad(encode(tgt, tgt_vocab), tgt_max_len)\n",
    "    data.append((src_ids, tgt_ids))\n",
    "\n",
    "# ---------------------------\n",
    "# Model Components \n",
    "# ---------------------------\n",
    "# d_model: Dimension of embedding vectors\n",
    "# num_heads: Number of attention heads\n",
    "# num_layers: Number of encoder and decoder layers\n",
    "# d_ff: Dimension of feed-forward network\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model) # initialize linear layers for query, key, value\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn, v)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        return self.out(output)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=256):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.linear2(torch.relu(self.linear1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.norm1(x + self.attn(x, x, x, mask))\n",
    "        return self.norm2(x + self.ff(x))\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        x = self.norm1(x + self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.norm2(x + self.cross_attn(x, enc_output, enc_output, src_mask))\n",
    "        return self.norm3(x + self.ff(x))\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=128, num_heads=4, num_layers=2, d_ff=256):\n",
    "        super().__init__()\n",
    "        self.src_embed = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embed = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.pos_enc(self.src_embed(src))\n",
    "        tgt = self.pos_enc(self.tgt_embed(tgt))\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src)\n",
    "        enc_output = src\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt = layer(tgt, enc_output)\n",
    "        return self.fc_out(tgt)\n",
    "\n",
    "# ---------------------------\n",
    "# Training Loop\n",
    "# ---------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Transformer(len(src_vocab), len(tgt_vocab)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for src_ids, tgt_ids in data:\n",
    "        src_tensor = torch.tensor([src_ids], device=device)\n",
    "        tgt_tensor = torch.tensor([tgt_ids], device=device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src_tensor, tgt_tensor[:, :-1])  # predict next token\n",
    "        loss = criterion(output.view(-1, len(tgt_vocab)), tgt_tensor[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Test Prediction\n",
    "# ---------------------------\n",
    "def predict(word):\n",
    "    src_ids = pad(encode(word, src_vocab), src_max_len) # Encode and pad input word (normalize length)\n",
    "    tgt_ids = [tgt_vocab[\"<sos>\"]] # Initialize target sequence with <sos> (predicting from scratch)\n",
    "    for _ in range(tgt_max_len-1):\n",
    "        tgt_tensor = torch.tensor([tgt_ids], device=device)\n",
    "        src_tensor = torch.tensor([src_ids], device=device)\n",
    "        output = model(src_tensor, tgt_tensor)\n",
    "        next_token = output[0, -1].argmax().item()\n",
    "        if next_token == tgt_vocab[\"<eos>\"]: # Stop if <eos> is predicted\n",
    "            break\n",
    "        tgt_ids.append(next_token)\n",
    "    return \"\".join(inv_tgt_vocab[i] for i in tgt_ids[1:]) # Decode predicted tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9f16009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate 'hello' -> tase\n"
     ]
    }
   ],
   "source": [
    "print(\"Translate 'hello' ->\", predict(\"cup\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e4fe74",
   "metadata": {},
   "source": [
    "### Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7accd",
   "metadata": {},
   "source": [
    "**✅ What is tgt_vocab and why these tokens?**\n",
    "\n",
    "\n",
    "- `<pad>`: Padding token \n",
    "Used to make all sequences the same length by filling empty positions with zeros.\n",
    "\n",
    "\n",
    "- `<sos>`: Start-of-sequence token \n",
    "Indicates the beginning of the target sentence during decoding.\n",
    "\n",
    "\n",
    "- `<eos>`: End-of-sequence token \n",
    "Marks the end of the sentence so the model knows when to stop generating.\n",
    "\n",
    "\n",
    "These special tokens are essential for sequence models because:\n",
    "\n",
    "- Padding ensures batches have uniform shape.\n",
    "- Start and end tokens help the model learn where sentences begin and end.\n",
    "\n",
    "---\n",
    "\n",
    "**✅ How vocabularies are built**\n",
    "\n",
    "We start with special tokens.\n",
    "Then we add characters from the dataset (English and Pig Latin words).\n",
    "Each character gets a unique integer ID.\n",
    "\n",
    "src_vocab might look like:\n",
    "```{\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"h\":3, \"e\":4, \"l\":5, \"o\":6, \"w\":7, ...}```\n",
    "\n",
    "tgt_vocab might look like:\n",
    "```{\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"e\":3, \"l\":4, \"o\":5, \"h\":6, \"a\":7, \"y\":8, ...}```\n",
    "\n",
    "---\n",
    "\n",
    "**✅ Encoding and Padding**\n",
    "\n",
    "\n",
    "- encode(\"hello\", src_vocab) → [1, 3, 4, 5, 5, 6, 2]\n",
    "(`<sos>` + h,e,l,l,o + `<eos>`)\n",
    "\n",
    "\n",
    "- pad([1,3,4,5,5,6,2], 10) → [1,3,4,5,5,6,2,0,0,0]\n",
    "Adds `<pad>` tokens to reach length 10.\n",
    "\n",
    "---\n",
    "\n",
    "**✅ Why do we need this?**\n",
    "\n",
    "Transformers work with fixed-length tensors for batching. Padding ensures:\n",
    "\n",
    "- All sequences have the same length.\n",
    "- The model ignores padding during attention (using masks).\n",
    "\n",
    "---\n",
    "\n",
    "**✅ Training Loop Summary**\n",
    "\n",
    "For each pair (English → French):\n",
    "\n",
    "- Encode and pad both source and target.\n",
    "- Feed source and target (except last token) into the model.\n",
    "- Predict next token and compute loss against target shifted by one position.\n",
    "- Repeat for multiple epochs.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
